{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from math import floor\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE, roc_auc_score as AUC_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from regression_problem import Regression\n",
    "\n",
    "mpl.rcdefaults()\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "mpl.rcParams['figure.figsize'] = [10.0, 4.0]\n",
    "mpl.rcParams['figure.dpi'] = 80\n",
    "mpl.rcParams['savefig.dpi'] = 100\n",
    "mpl.rcParams['font.size'] = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            X_data,\n",
    "            Y_data,\n",
    "            problem,    # input problem class, containing activiation functions and cost functions\n",
    "            n_hidden_neurons_list =[2],    # list with numbers of neurons in each layer\n",
    "            n_output_neurons=10,    # nr neurons output layer\n",
    "            epochs=10,\n",
    "            batch_size=100,\n",
    "            eta=0.1,\n",
    "            lmbd=0.0,\n",
    "            debug=False):\n",
    "\n",
    "        self.X_data_full = X_data\n",
    "        self.Y_data_full = Y_data\n",
    "\n",
    "        self.n_inputs = X_data.shape[0]\n",
    "        self.n_features = X_data.shape[1]\n",
    "        self.n_layers = len(n_hidden_neurons_list)\n",
    "        self.n_hidden_neurons_list = n_hidden_neurons_list\n",
    "        self.n_output_neurons = n_output_neurons\n",
    "\n",
    "        self.Problem = problem\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.iterations = self.n_inputs // self.batch_size\n",
    "        self.eta = eta\n",
    "        self.lmbd = lmbd\n",
    "        \n",
    "        self.debug = debug\n",
    "        self.accuracy_each_epoch_train = np.zeros(epochs)\n",
    "        self.accuracy_each_epoch_test = np.zeros(epochs)\n",
    "        self.area_under_curve_train = np.zeros(epochs)\n",
    "        self.area_under_curve_test = np.zeros(epochs)\n",
    "\n",
    "        self.initialize_layers()\n",
    "\n",
    "    def initialize_layers(self):\n",
    "        \"\"\" Initializes the weights and biases for the hidden layers and output layer \"\"\"\n",
    "        n_hidden = self.n_hidden_neurons_list\n",
    "        \n",
    "        # Bias of layers l = [1,L-1]:\n",
    "        self.bias_list = [np.zeros(n)+0.01 for n in n_hidden]\n",
    "        # appending output layer:\n",
    "        self.bias_list.append(np.zeros(self.n_output_neurons)+0.01)\n",
    "        \n",
    "        # Weights for l = [1,L-1]:\n",
    "        self.weights_list = [np.random.randn(self.n_features,n_hidden[0])]    # From input to l=1\n",
    "        # Dimension of layers l dependenent on layer l-1:\n",
    "        for i in range(1,self.n_layers):\n",
    "            self.weights_list.append(np.random.randn(n_hidden[i-1],n_hidden[i]))\n",
    "        # appending output layer:\n",
    "        self.weights_list.append(np.random.randn(n_hidden[-1], self.n_output_neurons))\n",
    "    \n",
    "    def printshape(self,x,name='x'):\n",
    "        \"\"\" Helper function for debugging, printing shapes of variable x \"\"\"\n",
    "        if isinstance(x,list):\n",
    "            x = np.array(x)\n",
    "        if self.debug:\n",
    "            print('shape '+name,x.shape)\n",
    "            \n",
    "    def feed_forward(self):\n",
    "        \"\"\"\n",
    "        Feed forward loop used in training, looping all hidden layers and\n",
    "        produces output probabilities for use in back propagation in last element of a\n",
    "        \"\"\"\n",
    "        problem = self.Problem\n",
    "        # Input, Not using weights and biases for input layer\n",
    "        self.a_list = [self.X_data]\n",
    "        self.z_list = []\n",
    "        \n",
    "        # Loop through the layers, store weighted sum and activations\n",
    "        for w,b in zip(self.weights_list,self.bias_list):\n",
    "            \n",
    "            self.z_list.append(np.matmul(self.a_list[-1],w)+b)\n",
    "            self.a_list.append(problem.hidden_activation(self.z_list[-1]))\n",
    "            \n",
    "        # Overwrite the last entry in a_list to use the output activation function\n",
    "        self.a_list[-1] = problem.output_activation(self.z_list[-1])\n",
    "\n",
    "    def feed_forward_out(self, X):\n",
    "        problem = self.Problem\n",
    "        # Input, Not using weights and biases for input layer\n",
    "        a_list = [X]\n",
    "        z_list = []\n",
    "\n",
    "        for w,b in zip(self.weights_list,self.bias_list):\n",
    "            z_list.append(np.matmul(a_list[-1],w)+b)\n",
    "            a_list.append(problem.hidden_activation(z_list[-1]))\n",
    "\n",
    "        # Overwrite last entry:\n",
    "        a_list[-1] = problem.output_activation(z_list[-1])\n",
    "        return a_list[-1]\n",
    "\n",
    "    def backpropagation(self):\n",
    "        \"\"\"\n",
    "        Performs the back propagation algorithm, with output from forward pass\n",
    "        in self.probabilities. Uses the expressions from the given Problem class\n",
    "        to compute the output error.\n",
    "        \"\"\"\n",
    "        problem = self.Problem\n",
    "        \n",
    "        ## First find errors from each layer: ##\n",
    "        ## ---------------------------------- ##\n",
    "        error_list = []\n",
    "        grad_w_list = []\n",
    "        grad_b_list = []\n",
    "        \n",
    "        # Finds the output error (delta^L) using the given cost function from Problem\n",
    "        self.printshape(self.a_list[-1],'aL')\n",
    "        self.printshape(self.Y_data,'Y')\n",
    "        self.printshape(self.z_list[-1],'zL')\n",
    "        output_error = problem.output_error(self.a_list[-1],self.Y_data,self.z_list[-1])\n",
    "        error_list.append(output_error)\n",
    "        \n",
    "        # Propagate error back in the hidden layers to find error from each layer\n",
    "        # (usually not so high nr of layers, so looping 2 times for easy implementation. Once for finding errors and once for applying them)\n",
    "        L = self.n_layers   # last layer number\n",
    "        \n",
    "        for l in range(2,L+2): \n",
    "            prev_error = error_list[-1]\n",
    "            prev_w = self.weights_list[-l+1]\n",
    "            current_z = self.z_list[-l]\n",
    "            self.printshape(prev_error,'error')\n",
    "            self.printshape(prev_w.T,'w.T')\n",
    "            error_hidden = np.matmul(prev_error,prev_w.T)*problem.hidden_activation(current_z,prime=True)  # CHECK ORDER IN MATMUL?\n",
    "            error_list.append(error_hidden)\n",
    "\n",
    "        # Error_list is optained backwards, reverting to match layer numbers better, but note there is no error for input layer\n",
    "        error_list.reverse()\n",
    "\n",
    "        ## Find gradients from each layer: ##\n",
    "        ## ------------------------------- ##\n",
    "        # Looping over all layers\n",
    "        for l in range(L+1):\n",
    "            \n",
    "            # Finding gradients\n",
    "            grad_b_list.append(np.sum(error_list[l],axis=0))\n",
    "            # Note that the index for a_list in the following seem to be wrong, but it is not.\n",
    "            # In the equations we have a[l-1] and delta [l], the way the error is calculated \n",
    "            # the indices of a_list and error_list are scewed once relative to each other!\n",
    "            grad_w_list.append(np.matmul(self.a_list[l].T,error_list[l]))\n",
    "\n",
    "            if self.lmbd > 0.0: # If using regularization\n",
    "                grad_w_list[l] += self.lmbd * self.weights_list[l]\n",
    "            \n",
    "            # Update weights and biases\n",
    "            self.weights_list[l] -= self.eta*grad_w_list[l]\n",
    "            self.bias_list[l] -= self.eta*grad_b_list[l]\n",
    "        \n",
    "    def get_prediction(self, X):\n",
    "        probabilities = self.feed_forward_out(X)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "    def get_predict_probabilities(self, X):\n",
    "        probabilities = self.feed_forward_out(X)\n",
    "        return probabilities\n",
    "\n",
    "    def SGD(self):\n",
    "        \"\"\"\n",
    "        Perform a stochastic gradient descent algorithm, looping over epochs and saturating each minibatch\n",
    "        Stores accuracy and area under curve score after each epoch, to vizualise the learning rate\n",
    "        \"\"\"\n",
    "        data_indices = np.arange(self.n_inputs)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            for j in range(self.iterations):\n",
    "                # pick datapoints with replacement\n",
    "                chosen_datapoints = np.random.choice(\n",
    "                    data_indices, size=self.batch_size, replace=False\n",
    "                )\n",
    "\n",
    "                # minibatch training data\n",
    "                self.X_data = self.X_data_full[chosen_datapoints]\n",
    "                self.Y_data = self.Y_data_full[chosen_datapoints]\n",
    "\n",
    "                self.feed_forward()\n",
    "                self.backpropagation()\n",
    "            \n",
    "            # Storing scores during the epochs\n",
    "            pred_test = self.get_predict_probabilities(X_test_scaled)\n",
    "            pred_train = self.get_predict_probabilities(X_train_scaled)\n",
    "            self.accuracy_each_epoch_test[i] = accuracy_score(from_one_hot(Y_test),np.argmax(pred_test,axis=1))\n",
    "            self.accuracy_each_epoch_train[i] = accuracy_score(from_one_hot(self.Y_data_full),np.argmax(pred_train,axis=1))\n",
    "            #self.area_under_curve_test[i] = AUC_score(Y_test,pred_test)\n",
    "            #self.area_under_curve_train[i] = AUC_score(Y_train,pred_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(Y_test, Y_pred):\n",
    "    return np.sum(Y_test == Y_pred) / len(Y_test)\n",
    "\n",
    "def to_one_hot(category_array):\n",
    "    ca = category_array # 1D array with values of the categories\n",
    "    nr_categories = np.max(ca)+1\n",
    "    nr_points = len(ca)\n",
    "    one_hot = np.zeros((nr_points,nr_categories),dtype=int)\n",
    "    one_hot[range(nr_points),ca] = 1\n",
    "    return one_hot\n",
    "\n",
    "def from_one_hot(one_hot_array):\n",
    "    category_arr = np.nonzero(one_hot_array)[1]\n",
    "    return category_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Happiness</th>\n",
       "      <th>Economy</th>\n",
       "      <th>Family</th>\n",
       "      <th>Health</th>\n",
       "      <th>Freedom</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Generosity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.587</td>\n",
       "      <td>1.396510</td>\n",
       "      <td>1.349510</td>\n",
       "      <td>0.941430</td>\n",
       "      <td>0.665570</td>\n",
       "      <td>0.419780</td>\n",
       "      <td>0.296780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.561</td>\n",
       "      <td>1.302320</td>\n",
       "      <td>1.402230</td>\n",
       "      <td>0.947840</td>\n",
       "      <td>0.628770</td>\n",
       "      <td>0.141450</td>\n",
       "      <td>0.436300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.527</td>\n",
       "      <td>1.325480</td>\n",
       "      <td>1.360580</td>\n",
       "      <td>0.874640</td>\n",
       "      <td>0.649380</td>\n",
       "      <td>0.483570</td>\n",
       "      <td>0.341390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.522</td>\n",
       "      <td>1.459000</td>\n",
       "      <td>1.330950</td>\n",
       "      <td>0.885210</td>\n",
       "      <td>0.669730</td>\n",
       "      <td>0.365030</td>\n",
       "      <td>0.346990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.427</td>\n",
       "      <td>1.326290</td>\n",
       "      <td>1.322610</td>\n",
       "      <td>0.905630</td>\n",
       "      <td>0.632970</td>\n",
       "      <td>0.329570</td>\n",
       "      <td>0.458110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>3.471</td>\n",
       "      <td>0.368746</td>\n",
       "      <td>0.945707</td>\n",
       "      <td>0.326425</td>\n",
       "      <td>0.581844</td>\n",
       "      <td>0.455220</td>\n",
       "      <td>0.252756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>3.462</td>\n",
       "      <td>0.777153</td>\n",
       "      <td>0.396103</td>\n",
       "      <td>0.500533</td>\n",
       "      <td>0.081539</td>\n",
       "      <td>0.151347</td>\n",
       "      <td>0.493664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>3.349</td>\n",
       "      <td>0.511136</td>\n",
       "      <td>1.041990</td>\n",
       "      <td>0.364509</td>\n",
       "      <td>0.390018</td>\n",
       "      <td>0.066035</td>\n",
       "      <td>0.354256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>2.905</td>\n",
       "      <td>0.091623</td>\n",
       "      <td>0.629794</td>\n",
       "      <td>0.151611</td>\n",
       "      <td>0.059901</td>\n",
       "      <td>0.084148</td>\n",
       "      <td>0.204435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>2.693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018773</td>\n",
       "      <td>0.270842</td>\n",
       "      <td>0.056565</td>\n",
       "      <td>0.280876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>470 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Happiness   Economy    Family    Health   Freedom     Trust  Generosity\n",
       "0        7.587  1.396510  1.349510  0.941430  0.665570  0.419780    0.296780\n",
       "1        7.561  1.302320  1.402230  0.947840  0.628770  0.141450    0.436300\n",
       "2        7.527  1.325480  1.360580  0.874640  0.649380  0.483570    0.341390\n",
       "3        7.522  1.459000  1.330950  0.885210  0.669730  0.365030    0.346990\n",
       "4        7.427  1.326290  1.322610  0.905630  0.632970  0.329570    0.458110\n",
       "..         ...       ...       ...       ...       ...       ...         ...\n",
       "465      3.471  0.368746  0.945707  0.326425  0.581844  0.455220    0.252756\n",
       "466      3.462  0.777153  0.396103  0.500533  0.081539  0.151347    0.493664\n",
       "467      3.349  0.511136  1.041990  0.364509  0.390018  0.066035    0.354256\n",
       "468      2.905  0.091623  0.629794  0.151611  0.059901  0.084148    0.204435\n",
       "469      2.693  0.000000  0.000000  0.018773  0.270842  0.056565    0.280876\n",
       "\n",
       "[470 rows x 7 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_names = [\"Happiness\", \"Economy\", \"Family\", \"Health\", \"Freedom\", \"Trust\", \"Generosity\"]\n",
    "data_df = pd.read_pickle(\"../data/world_happiness.pickle\")[parameter_names]\n",
    "data = data_df.to_numpy()\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(470, 6)\n",
      "(470,)\n",
      "(470, 1)\n"
     ]
    }
   ],
   "source": [
    "input_data = data[:,1:]\n",
    "output_data = data[:,0]\n",
    "print(input_data.shape)\n",
    "print(output_data.shape)\n",
    "output_data = output_data[:,np.newaxis]\n",
    "print(output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(1000)\n",
    "y = 3*x**2 + np.random.randn(1000)\n",
    "input_data = x.reshape(-1,1)\n",
    "output_data = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC mean =  0.0  accuracy mean =  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_neuron_list = [16,16]\n",
    "AUC = []\n",
    "accuracy = []\n",
    "epochs = 100\n",
    "runs = 10\n",
    "acc_test = np.zeros((runs,epochs))\n",
    "acc_train = np.zeros((runs,epochs))\n",
    "reg = Regression(hidden_activation='sigmoid',output_activation='sigmoid',cost_func='quadratic')\n",
    "\n",
    "for i in tqdm(range(runs)):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(input_data, output_data,test_size=0.1)\n",
    "    Scaler = preprocessing.StandardScaler()\n",
    "    X_train_scaled = Scaler.fit_transform(X_train)\n",
    "    X_test_scaled = Scaler.transform(X_test)\n",
    "    nn = NeuralNetwork( X_train_scaled,\n",
    "                        Y_train,\n",
    "                        problem = reg,\n",
    "                        n_hidden_neurons_list=hidden_neuron_list,\n",
    "                        n_output_neurons=1,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=256,\n",
    "                        eta=1e-3,\n",
    "                        lmbd=1.67,\n",
    "                        debug=False)\n",
    "    nn.SGD()\n",
    "    AUC.append(nn.area_under_curve_test[-1])\n",
    "    accuracy.append(nn.accuracy_each_epoch_test[-1])\n",
    "    acc_test[i,:] = nn.accuracy_each_epoch_test\n",
    "    acc_train[i,:] = nn.accuracy_each_epoch_train\n",
    "\n",
    "AUC_mean = np.mean(AUC)\n",
    "accuracy_mean = np.mean(accuracy)\n",
    "print('AUC mean = ',AUC_mean, ' accuracy mean = ',accuracy_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = nn.get_prediction(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
