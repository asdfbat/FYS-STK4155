{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import floor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from classification_problem import Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(Y_test, Y_pred):\n",
    "    return np.sum(Y_test == Y_pred) / len(Y_test)\n",
    "\n",
    "def to_one_hot(category_array):\n",
    "    ca = category_array # 1D array with values of the categories\n",
    "    nr_categories = np.max(ca)+1\n",
    "    nr_points = len(ca)\n",
    "    one_hot = np.zeros((nr_points,nr_categories),dtype=int)\n",
    "    one_hot[range(nr_points),ca] = 1\n",
    "    return one_hot\n",
    "\n",
    "def from_one_hot(one_hot_array):\n",
    "    category_arr = np.nonzero(one_hot_array)[1]\n",
    "    return category_arr\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            X_data,\n",
    "            Y_data,\n",
    "            problem,    # input problem class, containing activiation functions and cost functions\n",
    "            n_hidden_neurons_list =[2],    # list with numbers of neurons in each layer\n",
    "            n_categories=10,    # nr output layer\n",
    "            epochs=10,\n",
    "            batch_size=100,\n",
    "            eta=0.1,\n",
    "            lmbd=0.0,\n",
    "            debug=False):\n",
    "\n",
    "        self.X_data_full = X_data\n",
    "        self.Y_data_full = Y_data\n",
    "\n",
    "        self.n_inputs = X_data.shape[0]\n",
    "        self.n_features = X_data.shape[1]\n",
    "        self.n_layers = len(n_hidden_neurons_list)\n",
    "        self.n_hidden_neurons_list = n_hidden_neurons_list\n",
    "        self.n_categories = n_categories\n",
    "\n",
    "        self.Problem = problem\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.iterations = self.n_inputs // self.batch_size\n",
    "        self.eta = eta\n",
    "        self.lmbd = lmbd\n",
    "        \n",
    "        self.debug = debug\n",
    "        self.accuracy_each_epoch_train = np.zeros(epochs)\n",
    "        self.accuracy_each_epoch_test = np.zeros(epochs)\n",
    "\n",
    "        self.initialize_layers()\n",
    "\n",
    "    def initialize_layers(self):\n",
    "        # n_hidden = self.n_hidden_neurons_list\n",
    "        # self.hidden_bias_list = [np.zeros(n)+0.01 for n in n_hidden]\n",
    "        # self.hidden_weights_list = [np.random.randn(self.n_features,n_hidden[0])]    # Weight for first hidden layer(l=2)\n",
    "        # # Dimension of layers l dependenent on layer l-1 and number of \n",
    "        # for i in range(1,self.n_layers):\n",
    "        #     self.hidden_weights.append(np.random.randn(n_hidden[i-1],n_hidden[i]))\n",
    "\n",
    "        # self.output_weights = np.random.randn(self.n_hidden_neurons[-1], self.n_categories)\n",
    "        # self.output_bias = np.zeros(self.n_categories) + 0.01\n",
    "\n",
    "        n_hidden = self.n_hidden_neurons_list\n",
    "        # Bias of layers l = [1,L-1]:\n",
    "        self.bias_list = [np.zeros(n)+0.01 for n in n_hidden]\n",
    "        # appending output layer:\n",
    "        self.bias_list.append(np.zeros(self.n_categories)+0.01)\n",
    "        \n",
    "        # Weights for l = [1,L-1]:\n",
    "        self.weights_list = [np.random.randn(self.n_features,n_hidden[0])]    # From input to l=1\n",
    "        # Dimension of layers l dependenent on layer l-1:\n",
    "        for i in range(1,self.n_layers):\n",
    "            self.weights_list.append(np.random.randn(n_hidden[i-1],n_hidden[i]))\n",
    "        # appending output layer:\n",
    "        self.weights_list.append(np.random.randn(n_hidden[-1], self.n_categories))\n",
    "    \n",
    "    def printshape(self,x,name='x'):\n",
    "        if isinstance(x,list):\n",
    "            x = np.array(x)\n",
    "        if self.debug:\n",
    "            print('shape '+name,x.shape)\n",
    "            \n",
    "    def sigmoid(self,z):\n",
    "        \"\"\" Returns sigmoid activation function for hidden layers \"\"\"\n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "    def sigmoid_prime(self,z):\n",
    "        \"\"\" Returns derivative of sigmoid \"\"\"\n",
    "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
    "\n",
    "    def softmax(self,z):\n",
    "        \"\"\" Returns the softmax function \"\"\"\n",
    "        exp_term = np.exp(z)\n",
    "        return exp_term/np.sum(exp_term, axis=1, keepdims=True)\n",
    "\n",
    "    def feed_forward(self):\n",
    "        \"\"\"\n",
    "        Feed forward loop used in training, looping all hidden layers and\n",
    "        produces output probabilities for use in back propagation\n",
    "        \"\"\"\n",
    "        problem = self.Problem\n",
    "        # Input, Not using weights and biases for input layer\n",
    "        self.a_list = [self.X_data]\n",
    "        self.z_list = []\n",
    "\n",
    "        self.printshape(self.a_list[-1],'input')\n",
    "        \n",
    "        # Loop through the layers, store weighted sum and activations\n",
    "        i = 1\n",
    "        for w,b in zip(self.weights_list,self.bias_list):\n",
    "            \n",
    "            self.z_list.append(np.matmul(self.a_list[-1],w)+b)         #np.matmul(self.a_list[-1],w)+b)\n",
    "            self.a_list.append(self.sigmoid(self.z_list[-1]))\n",
    "            \n",
    "            self.printshape(w,'w'+str(i))\n",
    "            self.printshape(b,'b'+str(i))\n",
    "            self.printshape(self.a_list[-1],'a'+str(i))\n",
    "            i+=1\n",
    "\n",
    "        self.probabilities = self.softmax(self.z_list[-1])\n",
    "\n",
    "    def feed_forward_out(self, X):\n",
    "        problem = self.Problem\n",
    "        # Input, Not using weights and biases for input layer\n",
    "        a_list = [X]\n",
    "        z_list = []\n",
    "\n",
    "        for w,b in zip(self.weights_list,self.bias_list):\n",
    "            z_list.append(np.matmul(a_list[-1],w)+b)\n",
    "            a_list.append(self.sigmoid(z_list[-1]))\n",
    "\n",
    "        probabilities = self.softmax(z_list[-1])\n",
    "        return probabilities\n",
    "\n",
    "    def backpropagation(self):\n",
    "        \"\"\"\n",
    "        Performs the back propagation algorithm, with output from forward pass\n",
    "        in self.probabilities. Uses the expressions from the given Problem class\n",
    "        to compute the errors.\n",
    "        \"\"\"\n",
    "        ## Find errors from each layer: ##\n",
    "        ## ---------------------------- ##\n",
    "        error_list = []\n",
    "        # Finds the output error (delta^L) using the given cost function from Problem\n",
    "\n",
    "        self.printshape(self.probabilities,'prob',)\n",
    "        self.printshape(self.Y_data,'Y_data',)\n",
    "        \n",
    "        output_error = self.Problem.output_error(self.probabilities,self.Y_data)\n",
    "        error_list.append(output_error)\n",
    "        self.printshape(output_error,'output error')\n",
    "\n",
    "        # Propagate error back in the hidden layers to find error from each layer\n",
    "        L = self.n_layers   # last layer number\n",
    "                            # (usually not so high nr of layers, so looping multiple times for easy implementation)\n",
    "        for l in range(2,L): \n",
    "            prev_error = error_list[-1]\n",
    "            prev_w = self.weights_list[-l+1]\n",
    "            current_z = self.z_list[-l]\n",
    "            error_hidden = np.matmul(prev_error,prev_w.T)*self.sigmoid_prime(current_z)  # CHECK ORDER IN MATMUL?\n",
    "            error_list.append(error_hidden)\n",
    "\n",
    "        # Error_list is optained backwards, sorting to match layer numbers\n",
    "        error_list.reverse()\n",
    "        if self.debug:\n",
    "            print('number of errors ',len(error_list))\n",
    "\n",
    "        ## Find gradients from each layer: ##\n",
    "        ## ------------------------------- ##\n",
    "        # List for storing the gradient for each layer\n",
    "        grad_w_list = [np.zeros(w.shape) for w in self.weights_list]\n",
    "        grad_b_list = [np.zeros(b.shape) for b in self.bias_list]\n",
    "\n",
    "        # Looping over all layers\n",
    "        for l in range(self.n_layers):\n",
    "            \n",
    "            self.printshape(self.a_list[l].T,'a.T'+str(l))\n",
    "            self.printshape(error_list[l],'error'+str(l))\n",
    "            \n",
    "            # Finding gradients (storing for now)\n",
    "            grad_b_list[l] += np.sum(error_list[l],axis=0)\n",
    "            grad_w_list[l] += np.matmul(self.a_list[l+1].T,error_list[l])\n",
    "\n",
    "\n",
    "            if self.lmbd > 0.0: # If using regularization\n",
    "                grad_w_list[l] += self.lmbd * self.weights_list[l]\n",
    "    \n",
    "            self.printshape(self.weights_list[l],'w'+str(l))\n",
    "            self.printshape(grad_w_list[l],'grad_w'+str(l))\n",
    "            \n",
    "            # Update weights and biases\n",
    "            self.weights_list[l] -= self.eta*grad_w_list[l]\n",
    "            self.bias_list[l] -= self.eta*grad_b_list[l]\n",
    "        \n",
    "    def predict(self, X):\n",
    "        probabilities = self.feed_forward_out(X)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "    def predict_probabilities(self, X):\n",
    "        probabilities = self.feed_forward_out(X)\n",
    "        return probabilities\n",
    "\n",
    "    def train(self):\n",
    "        data_indices = np.arange(self.n_inputs)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            for j in range(self.iterations):\n",
    "                # pick datapoints with replacement\n",
    "                chosen_datapoints = np.random.choice(\n",
    "                    data_indices, size=self.batch_size, replace=False\n",
    "                )\n",
    "\n",
    "                # minibatch training data\n",
    "                self.X_data = self.X_data_full[chosen_datapoints]\n",
    "                self.Y_data = self.Y_data_full[chosen_datapoints]\n",
    "\n",
    "                self.feed_forward()\n",
    "                self.backpropagation()\n",
    "            self.accuracy_each_epoch_test[i] = accuracy_score(from_one_hot(Y_test),self.predict(X_test_scaled))\n",
    "            self.accuracy_each_epoch_train[i] = accuracy_score(from_one_hot(self.Y_data_full),self.predict(self.X_data_full))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def accuracy_score(Y_test, Y_pred):\n",
    "#    return np.sum(Y_test == Y_pred) / len(Y_test)\n",
    "#\n",
    "#def accuracy_score2(Y_test, Y_pred):\n",
    "#    return np.sum(np.argwhere(Y_test==1)[:,1] == Y_pred)/len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = np.arange(0, 1000).reshape(1000,1)\n",
    "output_data = np.zeros(len(input_data),dtype=int)\n",
    "output_data[floor(len(input_data)/2):] = 1\n",
    "output_one_hot = to_one_hot(output_data)\n",
    "nr_params = output_one_hot.shape[1]\n",
    "nr_datapoints = input_data.shape[0]; nr_datapoints,nr_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(input_data, output_onehot)\n",
    "Scaler = preprocessing.StandardScaler()\n",
    "X_train_scaled = Scaler.fit_transform(X_train)\n",
    "X_test_scaled = Scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape input (10, 1)\n",
      "shape w1 (1, 5)\n",
      "shape b1 (5,)\n",
      "shape a1 (10, 5)\n",
      "shape w2 (5, 4)\n",
      "shape b2 (4,)\n",
      "shape a2 (10, 4)\n",
      "shape w3 (4, 7)\n",
      "shape b3 (7,)\n",
      "shape a3 (10, 7)\n",
      "shape w4 (7, 2)\n",
      "shape b4 (2,)\n",
      "shape a4 (10, 2)\n",
      "shape prob (10, 2)\n",
      "shape Y_data (10, 2)\n",
      "shape output error (10, 2)\n",
      "number of errors  2\n",
      "shape a.T0 (1, 10)\n",
      "shape error0 (10, 7)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5,) (7,) (5,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-379-cd8ee25d5dbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0mlmbd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     debug=True)\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-374-42ce9ec17766>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_each_epoch_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_each_epoch_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_data_full\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_data_full\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-374-42ce9ec17766>\u001b[0m in \u001b[0;36mbackpropagation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;31m# Finding gradients (storing for now)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0mgrad_b_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0mgrad_w_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merror_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5,) (7,) (5,) "
     ]
    }
   ],
   "source": [
    "hidden_neuron_list = [5,4,7]\n",
    "nn = NeuralNetwork( X_train_scaled,\n",
    "                    Y_train,\n",
    "                    problem = Classification,\n",
    "                    n_hidden_neurons_list=hidden_neuron_list,\n",
    "                    n_categories=2,\n",
    "                    epochs=500,\n",
    "                    batch_size=10,\n",
    "                    eta=1e-5,\n",
    "                    lmbd=0.0,\n",
    "                    debug=True)\n",
    "nn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a259dea10>"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_pred = nn.predict(X_test_scaled)\n",
    "print(Y_pred.shape)\n",
    "#print(Y_pred)\n",
    "#print(accuracy_score(from_one_hot(Y_test),Y_pred))\n",
    "plt.figure()\n",
    "plt.plot(np.arange(nn.epochs),nn.accuracy_each_epoch_test,label='test')\n",
    "plt.plot(np.arange(nn.epochs),nn.accuracy_each_epoch_train,label='train')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.996"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def from_one_hot(one_hot_array):\n",
    "    category_arr = np.nonzero(one_hot_array)[1]\n",
    "    return category_arr\n",
    "\n",
    "Y_pred = nn.predict(X_test_scaled)\n",
    "accuracy_score(from_one_hot(Y_test),Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
